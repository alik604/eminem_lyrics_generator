{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "eminem_lyrics_generator - LSTM & HMM - Alik604",
      "provenance": [],
      "collapsed_sections": [
        "NnTDRtAZCt6H"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alik604/eminem_lyrics_generator/blob/master/eminem_lyrics_generator_LSTM_%26_HMM_Alik604.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8OTK5zjhteN"
      },
      "source": [
        "# By Alik604\n",
        "## Adapted from https://github.com/rojagtap/eminem_lyrics_generator\n",
        "> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rS6LXXTDcH6"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# cd /content/drive/My Drive/Data"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-w3FB4me4mr"
      },
      "source": [
        "%%capture\r\n",
        "\r\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "!unzip -q glove.6B.zip \r\n",
        "\r\n",
        "# !pip install hmmlearn pomegranate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7RNVjXQ-0ZK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import requests\n",
        "import codecs\n",
        "\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiYr_EWi4HkT"
      },
      "source": [
        "def clean_text(sentence):\n",
        "  sentence = sentence.lower()\n",
        "  sentence = re.sub(r'\\[.*?\\]', \"\", sentence) # https://www.regextester.com/97589 &  https://stackoverflow.com/a/40621332\n",
        "  sentence = re.sub(r'\\[0-9]', \"\", sentence)\n",
        "\n",
        "  sentence = re.sub(r\"\\u2005\", \"\", sentence)\n",
        "\n",
        "  sentence = re.sub(r\"’\", \"\\'\", sentence) \n",
        "  sentence = re.sub(r\"‘\", \"\\'\", sentence)\n",
        "  sentence = re.sub(r\"”\", \"\\'\", sentence) \n",
        "  sentence = re.sub(r\"“\", \"\\'\", sentence)\n",
        "\n",
        "  sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
        "  sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
        "  sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
        "  sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
        "  sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
        "  sentence = re.sub(r\"what's\", \"what is\", sentence)\n",
        "  sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
        "  sentence = re.sub(r\"there's\", \"there is\", sentence)\n",
        "  sentence = re.sub(r\"who's\", \"who is\", sentence)\n",
        "  sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
        "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "  sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
        "  sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
        "  sentence = re.sub(r\"n't\", \" not\", sentence)\n",
        "  sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
        "  sentence = re.sub(r\"\\'bout\", \"about\", sentence)\n",
        "  sentence = re.sub(r\"'til\", \"until\", sentence)\n",
        "  sentence = re.sub(r\"c'mon\", \"come on\", sentence)\n",
        "  sentence = re.sub(\"\\n\", \" \", sentence)\n",
        "\n",
        "  sentence = re.sub(r\"\\u2005\", \"\", sentence)\n",
        "  sentence = re.sub(\"[-*/()\\\"’‘'#/@;:<>{}`+=~|!?,]\", \"\", sentence) # should it be:  \\'   rather than '   # TODO note: removed . \n",
        "  sentence = re.sub(r\"'\", \"\", sentence)\n",
        "  sentence = re.sub(r\"\\t\", \"\", sentence)\n",
        "  sentence = re.sub(r\"\\r\", \"\", sentence)\n",
        "  sentence = re.sub(r\"\\n\", \"\", sentence)\n",
        "\n",
        "  sentence = re.sub(r\"  \", \" \", sentence)\n",
        "  sentence = re.sub(r\"  \", \" \", sentence)\n",
        "  sentence = re.sub(r\"  \", \" \", sentence)\n",
        "\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnTDRtAZCt6H"
      },
      "source": [
        "# Starting Preprocessing - Check point\n",
        "> This can be disreguarded as data is provided a single .csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4vc42jUjCgg"
      },
      "source": [
        "## retain\n",
        "\n",
        "# !pip install lyricsgenius\n",
        "# import lyricsgenius\n",
        "# genius = lyricsgenius.Genius(\"8AO32_nfbxk_8yNyBYJTnw_4qRjT2Uid1pMSW_XJ_56sBsnNLuXeKxwMMEZmQrQN\") # this is a valid key :) \n",
        "# artist = genius.search_artist(\"Eminem\", max_songs=100, sort='popularity') # XXXTENTACION \n",
        "# for i in range(100):\n",
        "#   artist.songs[i].save_lyrics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R94zWarKjCm1"
      },
      "source": [
        "# lines = []\n",
        "# i = 1\n",
        "# for root, dirs, files in os.walk('./raw'):\n",
        "#     for file in files:\n",
        "#         if file.endswith('.json') and 'xxxtentacion' in file : # eminem  # xxxtentacion\n",
        "#             file = open(\"./raw/\" + file)\n",
        "#             json_data = file.read()\n",
        "#             data = json.loads(json_data)\n",
        "#             lines.append(data['lyrics'])\n",
        "#             i +=1\n",
        "# print(i)\n",
        "# lines = list(filter(None, lines))\n",
        "# # lines\n",
        "\n",
        "\n",
        "# lines = pd.DataFrame(lines, columns=['lines'])\n",
        "# lines.lines = lines.lines.apply(lambda line: clean_text(line))\n",
        "# lines.values\n",
        "\n",
        "# lines.to_csv('./preprocessed_data_xxxtentacion.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypQd-Umh7jEj"
      },
      "source": [
        "# Starting ML - Check point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggswMub774Hh"
      },
      "source": [
        "## More books http://www.glozman.com/textpages.html\n",
        "\n",
        "\n",
        "## Eminem\n",
        "# lines = pd.read_csv('https://github.com/alik604/eminem_lyrics_generator/raw/master/data/preprocessed_data_eminem.csv',index_col=0)\n",
        "\n",
        "## XXXTentacion\n",
        "# lines = pd.read_csv('https://raw.githubusercontent.com/alik604/eminem_lyrics_generator/master/data/preprocessed_data_xxxtentacion.csv',index_col=0)\n",
        "\n",
        "## Treasure Island\n",
        "# raw_text_lower = requests.get(\"https://data.heatonresearch.com/data/t81-558/text/treasure_island.txt\").text.lower()\n",
        "# processed_text = re.sub(r'[^\\x00-\\x7f]',r'', raw_text_lower)\n",
        "# processed_text = clean_text(processed_text)\n",
        "# # processed_text\n",
        "# lines = pd.DataFrame(processed_text.split(\".\")[270:], columns=[\"lines\"])\n",
        "# print(lines.shape)\n",
        "# lines.drop_duplicates(inplace=True)\n",
        "# print(lines.shape)\n",
        "\n",
        "\n",
        "# Harry Potter books \n",
        "\n",
        "!git clone https://github.com/formcept/whiteboard.git\n",
        "# !ls whiteboard/nbviewer/notebooks/data/harrypotter\n",
        "\n",
        "DIR = \"whiteboard/nbviewer/notebooks/data/harrypotter\"\n",
        "\n",
        "temp = \"\"\n",
        "t = \"\"\n",
        "chars = []\n",
        "for book in os.listdir(DIR):\n",
        "    print(\"Reading \", book)\n",
        "    with codecs.open(DIR + '/' + book, \"rb\", \"utf-8\") as infile:\n",
        "        temp += infile.read()\n",
        "        chars.append(len(temp))\n",
        "        print(\"Characters read so far \" + str(len(temp)))\n",
        "\n",
        "processed_text = temp\n",
        "processed_text = processed_text.replace('Harry Potter and the Philosophers Stone - J.K. Rowling', '')\n",
        "processed_text = processed_text.replace('Harry Potter and the Chamber of Secrets - J.K. Rowling ', '')\n",
        "processed_text = processed_text.replace('Harry Potter and the Prisoner of Azkaban - J.K. Rowling ', '')\n",
        "processed_text = processed_text.replace('Harry Potter and the Goblet of Fire - J.K. Rowling ', '')\n",
        "processed_text = processed_text.replace('Harry Potter and the Order of the Phoenix -J.K. Rowling', '')\n",
        "processed_text = processed_text.replace('Harry Potter and the Half Blood Prince - J.K. Rowling', '')\n",
        "processed_text = processed_text.replace('Harry Potter and the Deathly Hallows - J.K. Rowling', '')\n",
        "processed_text = processed_text.replace('\\nJ ', '').replace(' K ', '').replace(' R O W L ! N G \\n\\nHARRY \\n\\nPOTTER', '').replace('Rowling', '')#.replace('Harry Potter and the', '')# .replace('Page | ', '')  \n",
        "processed_text = clean_text(processed_text)\n",
        "print(len(temp), ' char count to: ', len(processed_text))\n",
        "\n",
        "lines = pd.DataFrame(processed_text.split(\"page\"), columns=[\"lines\"])\n",
        "print('pre drop_duplicates', lines.shape)\n",
        "lines = lines.apply(lambda x: x.str.strip()) \n",
        "lines.drop_duplicates(inplace=True)\n",
        "print('post drop_duplicates', lines.shape)\n",
        "\n",
        "\n",
        "lines.head(10).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX8mMxFv-0a5"
      },
      "source": [
        "x_train = [line[:-1] for line in lines.lines]\n",
        "y_train = [line[1:] for line in lines.lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6cueFD--0bb"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT51cTsN-0bm"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines.lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyZARYVe-0b-"
      },
      "source": [
        "print(x_train[0])\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "y_train = tokenizer.texts_to_sequences(y_train)\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JivHjswYQqsD"
      },
      "source": [
        "word2idx = tokenizer.word_index\n",
        "idx2word = {value: key for key, value in word2idx.items()}\n",
        "\n",
        "word2idx[\"<pad>\"] = 0\n",
        "idx2word[0] = \"<pad>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYmlGZ8E-0cQ"
      },
      "source": [
        "lengths = [len(sequence) for sequence in x_train]\n",
        "\n",
        "lengths = pd.Series(lengths)\n",
        "lengths.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRip8p9Oh5HZ"
      },
      "source": [
        "# max(map(len, x_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRKUH4le1xo"
      },
      "source": [
        "# On to the Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY_uinnF-0co"
      },
      "source": [
        "maxlen = 256 # lengths.describe()['max'] # None to Infer it \n",
        "# maxlen: Optional Int, maximum length of all sequences. If not provided, sequences will be padded to the length of the longest individual sequence\n",
        "x_train = pad_sequences(x_train, maxlen=int(maxlen), padding='pre', truncating='pre') # prehaps pre is ideal: https://stackoverflow.com/a/51825971\n",
        "y_train = pad_sequences(y_train, maxlen=int(maxlen), padding='pre', truncating='pre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8GkHpsH-0c4"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import GRU, LSTM, Dense, Input, Embedding, Dropout, Bidirectional\n",
        "from tensorflow.keras.layers import * \n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-Pf1JSFeBf1"
      },
      "source": [
        "def generate(word):\n",
        "    # word = clean_text(word)\n",
        "    inputs = np.zeros((1, 1))\n",
        "    inputs[0, 0] = word2idx[word]\n",
        "    count = 1\n",
        "    while count <= 50:\n",
        "        pred = model.predict(inputs)\n",
        "        word = np.argmax(pred)\n",
        "        if word >= vocab_size:\n",
        "            word = vocab_size - 1\n",
        "\n",
        "        inputs[0, 0] = word\n",
        "        \n",
        "        print(idx2word[word], end=\" \")\n",
        "        count += 1\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    word = \"life\"\n",
        "    generate(word)\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyn7ULrweBff"
      },
      "source": [
        "path_to_glove_file = os.path.join(\"glove.6B.300d.txt\")\r\n",
        "\r\n",
        "embeddings_index = {}\r\n",
        "with open(path_to_glove_file) as f:\r\n",
        "    for line in f:\r\n",
        "        word, coefs = line.split(maxsplit=1)\r\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\r\n",
        "        embeddings_index[word] = coefs\r\n",
        "\r\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK01uVg3xcm9"
      },
      "source": [
        "# type(tokenizer.index_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vk9y788eTs2"
      },
      "source": [
        "num_tokens = len(tokenizer.word_index) + 2\r\n",
        "embedding_dim = 300\r\n",
        "hits = 0\r\n",
        "misses = 0\r\n",
        "\r\n",
        "# Prepare embedding matrix\r\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\r\n",
        "for i, word in tokenizer.index_word.items():\r\n",
        "    embedding_vector = embeddings_index.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "        # Words not found in embedding index will be all-zeros.\r\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\r\n",
        "        embedding_matrix[i] = embedding_vector\r\n",
        "        hits += 1\r\n",
        "    else:\r\n",
        "        misses += 1\r\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaEAkQ1T-0dE"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1 # 8526 for eminem; ~2000 for X\n",
        "# embedding_dim = 512 #1024 # 128\n",
        "bridge = int(vocab_size/3)\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ") # mask_zero=True\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(embedding_layer) # (batch_size, input_length) - > (batch_size, input_length, output_dim) \n",
        "model.add((Bidirectional(LSTM(embedding_dim, return_sequences=True, recurrent_regularizer=l2(0.01)))))\n",
        "\n",
        "model.add(GaussianNoise(0.2))\n",
        "# model.add(Dropout(0.6))\n",
        "\n",
        "# model.add(Dense(embedding_dim)) # I found this to work well in LSTMs for regression\n",
        "# model.add(Bidirectional(LSTM(embedding_dim, return_sequences=True)))\n",
        "\n",
        "model.add(Dense(vocab_size))\n",
        "\n",
        "opt = SGD(0.01) \n",
        "# Adam(0.001) is default\n",
        "# SGD(0.01) is default\n",
        "model.compile(optimizer=opt, loss=SparseCategoricalCrossentropy(from_logits=True)) # normally, I would have this set to false, and have my output layer have h <- sigmoid; were h is the activation function\n",
        "\n",
        "best = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-afmZUd-0dL"
      },
      "source": [
        "# y_train.shape\n",
        "history = model.fit(x_train, y_train, epochs=3, batch_size=32, verbose=1, callbacks=[best, print_callback], validation_split=0.1).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF9Z5xlkQ3Nm"
      },
      "source": [
        "# model.save(\"model_BiLstm_Harry_potter.h5\")\n",
        "#model = load_model(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQoTjmtY-0dh"
      },
      "source": [
        "generate(\"the\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWoOF4cwkp3l"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(history['loss'])), history['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7oEhWaTz0V-"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=3, batch_size=256, verbose=1, callbacks=[best, print_callback], validation_split=0.1).history\r\n",
        "generate(\"the\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMKh25LdeBgA"
      },
      "source": [
        "## Try A Hidden Markov Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU17FZhR_-j8"
      },
      "source": [
        "from hmmlearn.hmm import MultinomialHMM\n",
        "print(len(tokenizer.word_index)) # vocab size? \n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9KiixB0_-_c"
      },
      "source": [
        "hmm = MultinomialHMM(n_components=10, n_iter = 100) # covariance_type=\"full\"\n",
        "hmm.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDB9wpNvEtcj"
      },
      "source": [
        "generated, y  = hmm.sample(20)  # np.array([1,2,3]).reshape(-1, 1)\n",
        "generated = generated.flatten()\n",
        "print(generated)\n",
        "\n",
        "for pred in generated:\n",
        "  print(idx2word[pred], end =' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHMAwdCoeBgF"
      },
      "source": [
        "### Train on 1D representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehvRLYh_E-ES"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_train.flatten().shape)\n",
        "\n",
        "hmm = MultinomialHMM(n_components=10, n_iter = 300) # covariance_type=\"full\"\n",
        "hmm.fit(x_train.flatten().reshape(-1, 1)) # max: 60000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqCOAZUpXfBv"
      },
      "source": [
        "generated, y = hmm.sample(20)  # np.array([1,2,3]).reshape(-1, 1)\n",
        "generated = generated.flatten()\n",
        "print(generated)\n",
        "for pred in generated:\n",
        "  print(idx2word[pred], end =' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPPAuXfKeBgJ"
      },
      "source": [
        "## Try the package [Pomegranate](https://pomegranate.readthedocs.io/en/latest/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwA7Q5WneBgJ"
      },
      "source": [
        "from pomegranate import *\n",
        "\n",
        "data_sample = x_train[:10]\n",
        "data_sample.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE1pZ779eBgL"
      },
      "source": [
        "model = MarkovNetwork.from_samples(data_sample, n_jobs = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RIRASFleBgM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V207Xbq3eBgQ"
      },
      "source": [
        "# model = BayesianNetwork.from_samples(data_sample, n_jobs = 3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgSPeAwTeBgR"
      },
      "source": [
        "model.predict(data_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGpH9MoWeBgT"
      },
      "source": [
        "# MultivariateGaussianDistribution will cause crash. may data is the worng shape\n",
        "# model = HiddenMarkovModel.from_samples(distribution = TODO, X= data_sample, n_components = 5, max_iterations=1)\n",
        "# model.bake()\n",
        "# model.fit(data_sample)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsHduTTTeBgV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP6wGzQ4FbMI"
      },
      "source": [
        "#### Debuging "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIrKHW4WHswT"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DARIusabIPMh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OHpZuGwb_1k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}